{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  text classification model \n",
    "\n",
    " model used to automatically categorize text documents into predefined classes or categories based on their content. The goal of text classification is to assign one or more labels to each document, indicating its category or class.\n",
    "\n",
    " Example - Transformers sentiment-analysis models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts :  ['This movie is great!', \"I didn't like this film.\", 'The acting was superb.'] \n",
      "Labels :  [1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Read input dataset and in this dataset label represents ->\n",
    "# 1-> positive comment\n",
    "# 0-> negitive comment\n",
    "# 2-> neutral comment\n",
    "import json\n",
    "with open(\"./support_files/text_classification_dataset.json\") as f:\n",
    "    initial_data = json.load(f)\n",
    "\n",
    "# Seprating texts and labels\n",
    "texts = [data[\"text\"] for data in initial_data]\n",
    "labels = [data[\"label\"] for data in initial_data]\n",
    "\n",
    "print(\"Texts : \",texts[:3], \"\\nLabels : \",labels[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens :  [[0, 1, 2, 3], [4, 5, 6, 7, 8], [9, 10, 11, 12]]\n",
      "Max Length :  10 \n",
      "Padding Example :  [[0, 1, 2, 3, 0, 0, 0, 0, 0, 0], [4, 5, 6, 7, 8, 0, 0, 0, 0, 0], [9, 10, 11, 12, 0, 0, 0, 0, 0, 0]]\n",
      "X :  tensor([[ 0,  1,  2,  3,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4,  5,  6,  7,  8,  0,  0,  0,  0,  0],\n",
      "        [ 9, 10, 11, 12,  0,  0,  0,  0,  0,  0]]) \n",
      "y :  tensor([1, 0, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "__main__.TextClassifier"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Tokenizations and vocabulary\n",
    "# We can use transformers AutoTokenizer to create tokens instead as well!\n",
    "word_to_idx = {}\n",
    "idx_to_word = {}\n",
    "idx = 0\n",
    "for text in texts:\n",
    "    for word in text.split():\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = idx\n",
    "            idx_to_word[idx] = word\n",
    "            idx += 1\n",
    "\n",
    "# Convert texts to sequences of word indices\n",
    "sequences = [[word_to_idx[word] for word in text.split()] for text in texts]\n",
    "print(\"Tokens : \",sequences[:3])\n",
    "\n",
    "# Padding sequences to the same length to help with efficient training {Added 0 to eralize the length}\n",
    "max_seq_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = [seq + [0] * (max_seq_length - len(seq)) for seq in sequences]\n",
    "print(\"Max Length : \",max_seq_length,\"\\nPadding Example : \",padded_sequences[:3])\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "# Tensors are multi-dimensional arrays used to represent data in deep learning models. \n",
    "# They have a rank, shape, and data type, and support various mathematical operations. \n",
    "# Tensors are fundamental to building and training neural networks, \n",
    "# serving as the main data structure for input data and model parameters.\n",
    "X = torch.tensor(padded_sequences, dtype=torch.long)\n",
    "y = torch.tensor(labels, dtype=torch.long)\n",
    "print(\"X : \",X[:3],\"\\ny : \",y[:3])\n",
    "\n",
    "# Creating a simple dataset class\n",
    "# This is torch dataset which we will use to train torch model\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Split data into train and test sets using sklearn train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# now we will create data loaders and datasets\n",
    "train_dataset = TextDataset(X_train, y_train)\n",
    "test_dataset = TextDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Model definition\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=1, dropout=0.2):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        # This is first layer which is creating embedings, \n",
    "        # We can use sentence_transformers and directly pass embedings to prevent overhead\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) \n",
    "        # this is 2nd layer which is LSTM (Long Short-Term Memory) based on RNN\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout) \n",
    "        # this is fully connected layer it performs a linear transformation on the input data.\n",
    "        # It consists of weights and biases that are learned during the training process.\n",
    "        # The linear layer projects the output of the LSTM layer to the desired output dimension (output_dim), which represents the number of classes in the classification task.\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        # Dropout is a regularization technique used to prevent overfitting in neural networks by randomly dropping (zeroing out) a proportion of input units during training.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # In PyTorch neural network modules, \n",
    "    # the forward method defines the computation performed when the module is called with input data. \n",
    "    # It outlines how the input data flows through the layers of the network to produce the output.\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        # Take the output from the last timestep\n",
    "        output = output[:, -1, :]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "TextClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.0219\n",
      "Test Accuracy: 28.57%\n",
      "\n",
      "Epoch [2/10], Loss: 0.8568\n",
      "Test Accuracy: 28.57%\n",
      "\n",
      "Epoch [3/10], Loss: 0.8280\n",
      "Test Accuracy: 28.57%\n",
      "\n",
      "Epoch [4/10], Loss: 0.7503\n",
      "Test Accuracy: 28.57%\n",
      "\n",
      "Epoch [5/10], Loss: 0.6591\n",
      "Test Accuracy: 42.86%\n",
      "\n",
      "Epoch [6/10], Loss: 0.5694\n",
      "Test Accuracy: 42.86%\n",
      "\n",
      "Epoch [7/10], Loss: 0.4380\n",
      "Test Accuracy: 42.86%\n",
      "\n",
      "Epoch [8/10], Loss: 0.1532\n",
      "Test Accuracy: 57.14%\n",
      "\n",
      "Epoch [9/10], Loss: 0.0672\n",
      "Test Accuracy: 57.14%\n",
      "\n",
      "Epoch [10/10], Loss: 0.0507\n",
      "Test Accuracy: 28.57%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "vocab_size = len(word_to_idx)\n",
    "embedding_dim = 100 # Vectors dimenssions\n",
    "hidden_dim = 128\n",
    "output_dim = 3  # Three classes: positive 1, negative 0, neutral 2\n",
    "num_layers = 2  # Increase number of LSTM layers\n",
    "dropout = 0.5  # Add dropout for regularization\n",
    "\n",
    "# Instantiate the model\n",
    "model = TextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "# An epoch is a single iteration through the entire training dataset during neural network training. \n",
    "# It involves forward and backward passes, where the model makes predictions, computes the loss, and updates parameters. \n",
    "# Multiple epochs are typically used to ensure the model learns from the entire dataset multiple times for better convergence and performance.\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    confusion_matrix = np.zeros((output_dim, output_dim), dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            for t, p in zip(target.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "    # print(\"Confusion Matrix:\\n\", confusion_matrix)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive :  It was good.\n",
      "Negative :  It was a terrible experience.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# inference\n",
    "# to use the trained model\n",
    "# For example, to predict the sentiment of a new text\n",
    "def predict_sentiment(text):\n",
    "    # Preprocess the text (tokenization, padding, etc.)\n",
    "    sequence = [word_to_idx.get(word, 0) for word in text.split()]  # Assign index 0 for OOV words\n",
    "    padded_sequence = sequence + [0] * (max_seq_length - len(sequence))  # Pad sequence\n",
    "    input_tensor = torch.tensor([padded_sequence], dtype=torch.long)\n",
    "\n",
    "    # Pass the input tensor through the model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    # Get predicted label\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    predicted_label = \"Positive\" if predicted.item() else (\"Neutral\" if predicted.item()==2 else \"Negative\")\n",
    "    return predicted_label\n",
    "\n",
    "# Example usage\n",
    "new_text = \"It was good.\"\n",
    "predicted_sentiment = predict_sentiment(new_text)\n",
    "print(predicted_sentiment, \": \", new_text)\n",
    "\n",
    "new_text = \"It was a terrible experience.\"\n",
    "predicted_sentiment = predict_sentiment(new_text)\n",
    "print(predicted_sentiment, \": \", new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifier(\n",
       "  (embedding): Embedding(103, 100)\n",
       "  (rnn): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (fc): Linear(in_features=128, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"trained_data/base_text_classifier_model.pth\"\n",
    "\n",
    "# Save the model state dict\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model = TextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "# Load the saved model state dict\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()  # Set the model to evaluation mode to text it later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipping Embeddings?\n",
    "\n",
    "We can skip embedding layer in model by passing embeddings in our model directly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "Sample :  [-0.06804912 -0.0057736  -0.00555508]\n",
      "Embedding size :  384\n"
     ]
    }
   ],
   "source": [
    "# We can create a model with no embeding step and use sentence transformer to help us creating embeding instead\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "emb_model = SentenceTransformer(\"avsolatorio/GIST-small-Embedding-v0\", device=\"cuda\")\n",
    "print(emb_model)\n",
    "sentence_embeddings = emb_model.encode(texts, batch_size=8)\n",
    "print(\"Sample : \",sentence_embeddings[0][:3])\n",
    "print(\"Embedding size : \",sentence_embeddings.shape[1])\n",
    "\n",
    "# Create tensor and datasets\n",
    "# We can use previous TextDataset(...)\n",
    "X = torch.tensor(sentence_embeddings, dtype=torch.float32)\n",
    "y = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TextDataset(X_train, y_train)\n",
    "test_dataset = TextDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.0604\n",
      "Test Accuracy: 57.14%\n",
      "\n",
      "Epoch [2/5], Loss: 1.0396\n",
      "Test Accuracy: 57.14%\n",
      "\n",
      "Epoch [3/5], Loss: 0.9936\n",
      "Test Accuracy: 57.14%\n",
      "\n",
      "Epoch [4/5], Loss: 0.9557\n",
      "Test Accuracy: 42.86%\n",
      "\n",
      "Epoch [5/5], Loss: 0.8775\n",
      "Test Accuracy: 42.86%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model definition without embedding layer\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.2):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x.unsqueeze(1))  # Add an extra dimension for sequence length\n",
    "        # Take the output from the last timestep\n",
    "        output = output[:, -1, :]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# Model parameters\n",
    "input_dim = sentence_embeddings.shape[1]\n",
    "hidden_dim = 128\n",
    "output_dim = 3  # Three classes: positive, negative, neutral\n",
    "num_layers = 2  # Increase number of LSTM layers\n",
    "dropout = 0.5  # Add dropout for regularization\n",
    "\n",
    "# Instantiate the model\n",
    "model = TextClassifier(input_dim, hidden_dim, output_dim, num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    confusion_matrix = np.zeros((output_dim, output_dim), dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            for t, p in zip(target.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "    # print(\"Confusion Matrix:\\n\", confusion_matrix)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: 1\n"
     ]
    }
   ],
   "source": [
    "# Define the inference function\n",
    "def predict_sentiment(text):\n",
    "    # Encode input text\n",
    "    input_embedding = emb_model.encode([text])\n",
    "    input_tensor = torch.tensor(input_embedding, dtype=torch.float32)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    output = model(input_tensor)\n",
    "\n",
    "    # Get predicted class\n",
    "    _, predicted_class = torch.max(output, 1)\n",
    "\n",
    "    return predicted_class.item()\n",
    "\n",
    "# Example usage\n",
    "new_text = \"It is a good movie\"\n",
    "predicted_sentiment = predict_sentiment(new_text)\n",
    "print(\"Predicted sentiment:\", predicted_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
