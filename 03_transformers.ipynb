{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd054a8f-4bdd-47bb-88bd-dbce2720bb66",
   "metadata": {},
   "source": [
    "### Transformers Pipelines\n",
    "\n",
    "The pipeline allows us to perform common NLP tasks such as text classification, named entity recognition (NER), question answering, summarization, translation, and more, with just a few lines of code. It automatically handles tasks such as tokenization, model loading, inference, and post-processing.\n",
    "\n",
    "We can use pretrained models and can add out custom models\n",
    "\n",
    "#### important args/params\n",
    "\n",
    "there are some important params of transformers pipeline such as \n",
    "- task: NLP taks like sentiment-analysis, text-generation...\n",
    "- model: pre trained model name...\n",
    "- tokenizer: token genrator model\n",
    "- feature_extractor: feature extractor model\n",
    "- framework: pytorch or tf\n",
    "- device: cpu/gpu(CUDA)\n",
    "- max_length: max length on which computation will happen rest will be truncated, it is necessary to control memory usage during inference\n",
    "\n",
    "#### how it works?\n",
    "\n",
    "Transformers library's pipeline can be understood as a high-level abstraction that encapsulates the steps involved in using pre-trained models for various natural language processing (NLP) tasks.\n",
    "\n",
    "#### it's components :-\n",
    "\n",
    "- Task-Specific Model Loading: The pipeline selects and loads a pre-trained model that is specifically designed for the chosen NLP task. For example, if the task is sentiment analysis, the pipeline loads a pre-trained model that has been fine-tuned on sentiment analysis tasks.\n",
    "\n",
    "- Tokenization: The input text is tokenized using the tokenizer associated with the loaded model. Tokenization involves breaking down the input text into smaller units such as words, subwords, or characters, depending on the tokenizer's configuration.\n",
    "\n",
    "- Inference: The tokenized input is fed into the loaded model for inference. The model processes the input tokens through its layers, applying various transformations and computations to generate predictions or outputs specific to the chosen task.\n",
    "\n",
    "- Post-Processing: The model's outputs are post-processed as necessary to obtain the final result. This may involve converting model outputs into human-readable formats, aggregating multiple outputs, or performing additional processing steps depending on the task requirements.\n",
    "\n",
    "- Output: The final output of the pipeline is returned to the user. This output typically includes the predictions or results of the NLP task, such as sentiment labels, named entities, answers to questions, summaries of text, translations, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c238047",
   "metadata": {},
   "source": [
    "### Tasks Pretrained Models\n",
    "\n",
    "refer this to get all pre tained model and tasks https://huggingface.co/tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfcef6da-9951-4a12-86ba-9e27e9ba941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e7266a-5393-49d9-b21e-ee1107a990b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\satya\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998718500137329},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997977614402771},\n",
       " {'label': 'POSITIVE', 'score': 0.9979287385940552}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sementic analyzer\n",
    "# Sentiment analysis is a natural language processing (NLP) technique used to determine the sentiment or emotional tone expressed in a piece of text. \n",
    "# It involves analyzing text data to identify and classify opinions, emotions, attitudes, or sentiments conveyed by the author. \n",
    "classifier = pipeline(\"sentiment-analysis\",\n",
    "                      model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", \n",
    "                      revision=\"af0f99b\")\n",
    "\n",
    "results = classifier(\n",
    "    [\n",
    "        \"this is a positive thing to do.\",\n",
    "        \"wat you did was not a good thing to do at all!....\",\n",
    "        \"what you did wasn't bad.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a4e4c59-e3b3-4f20-aed5-cf2a06d5c583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': 'Once upon a time, it became quite clear.\\n\\n\"She\\'s a young girl. She\\'s a woman of twenty'}],\n",
       " [{'generated_text': 'He was a good guy,\" White said in his statement.\\n\\nWhite was a teammate of New York Giants coach Bruce B'}]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text generator\n",
    "\n",
    "generator = pipeline(\"text-generation\",\n",
    "                     model=\"openai-community/gpt2\", \n",
    "                     revision=\"6c0e608\")\n",
    "\n",
    "results = generator([\n",
    "    \"Once upon a time\",\n",
    "    \"He was a good guy\"\n",
    "],max_length=25, truncation=True)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2c6c22b-ff03-488c-8082-dcc6f726bd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'orignal': \"Large Language Models (LLMs) represent a groundbreaking advancement in natural language processing (NLP), revolutionizing the way machines understand and generate human-like text. These models, powered by deep learning algorithms and massive amounts of training data, have demonstrated remarkable capabilities in various NLP tasks, including text generation, translation, sentiment analysis, and more. At the forefront of LLMs are architectures like OpenAI's GPT (Generative Pre-trained Transformer) series, Google's BERT (Bidirectional Encoder Representations from Transformers), and other transformer-based models.\"}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Large Language Models (LLMs) represent a groundbreaking advancement in natural language processing (NLP) These models are powered by deep learning algorithms and massive amounts of training data . LLMs have demonstrated remarkable capabilities in various NLP tasks, including text generation, translation, sentiment'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text summarizer\n",
    "\n",
    "summarizer = pipeline(\"summarization\",\n",
    "                     model=\"sshleifer/distilbart-cnn-12-6\", \n",
    "                     revision=\"a4f8f3e\")\n",
    "import sys \n",
    "# Reading some long text\n",
    "with open('support_files/long_text.txt', 'r') as file:\n",
    "    contents = file.read()\n",
    "print([{\"orignal\":contents}])\n",
    "\n",
    "result = summarizer(contents, max_length=56)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0792bdc7-0885-4f89-b5b1-057cf5681703",
   "metadata": {},
   "source": [
    "### Transformers Tokenizers\n",
    "\n",
    "\n",
    "Tokenizers are essential in NLP for preprocessing text, segmenting words, standardizing representations, managing vocabularies, handling special tokens, utilizing subword tokenization, and ensuring computational efficiency. They play a crucial role in transforming raw text into a format suitable for analysis and input into NLP models.\n",
    "\n",
    "We can use Pre trained tokens or we can create our own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ad20f64-2da1-455f-95b6-c6b64c1780f1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using pre trained tokenizers!\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# AutoTokenizer will automatically select the appropriate tokenizer based on the model name\n",
    "# We will use bert-base-uncased\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75f77a4b-e2f7-45e7-8a90-0d077352afef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2023, 2003, 1037, 15488, 9331, 2571, 3793, 1024, 1007, 102]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some text we want to create tokens for \n",
    "text = \"This is a smaple text :)\"\n",
    "\n",
    "# Tokenize the input text\n",
    "encoded_input = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49338fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a smaple text : )'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can decode the tokens using decode function\n",
    "decoded_text = tokenizer.decode(encoded_input, skip_special_tokens=True)\n",
    "\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4922b89",
   "metadata": {},
   "source": [
    "### Creating our own tokenizer\n",
    "\n",
    "To create our own tokenizer we will use BPE Byte Pair Encoding\n",
    "\n",
    "It is a popular subword tokenization technique used in natural language processing (NLP) tasks.\n",
    "\n",
    "In BPE, the input text is segmented into variable-length subword units. The segmentation is performed iteratively by merging the most frequent pairs of adjacent characters or character sequences. This process continues until a predefined vocabulary size is reached or until the desired number of merge operations is completed.\n",
    "\n",
    "BPE effectively handles rare words, out-of-vocabulary words, and morphologically rich languages by allowing the model to learn subword representations that can capture meaningful linguistic patterns. It has been widely adopted in various NLP applications, including machine translation, text generation, and sentiment analysis, among others, due to its flexibility and effectiveness in handling different types of text data.\n",
    "\n",
    "reffer https://huggingface.co/docs/tokenizers/en/components#models for more info on other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f943040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.Tokenizer at 0x1a458f3c150>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will import Tokenizer BPE and other necessary imports\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Creating tokenizer\n",
    "# We are using BPE and unk_token=[UNK] as we want to specifies the token to use for unknown words.\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3745578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.trainers.BpeTrainer at 0x1a45acab570>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will now create a trainer to train our tokenizer\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "# set pre_tokenizer to whitespace to tokenize text based on whitespaces\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "584be4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will download some datasets to train our tokenizer modle from hugging face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# we will use wikitext-2-raw-v1\n",
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\")\n",
    "\n",
    "# We will have test, train and validation dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "095529fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['', ' = Valkyria Chronicles III = \\n', ''],\n",
       " ['', ' = Robert Boulter = \\n', ''],\n",
       " ['', ' = Homarus gammarus = \\n', ''])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset sample\n",
    "dataset[\"train\"][:3][\"text\"], dataset[\"test\"][:3][\"text\"], dataset[\"validation\"][:3][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ffb38ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the dataset to a file as tokenizer.train take two arguments files and trainer\n",
    "dataset_path = \"support_files/temp/dummy_dataset\"\n",
    "\n",
    "# Iterate over the dataset and save each example as a raw text file\n",
    "for split in dataset.keys():\n",
    "    split_data = dataset[split]\n",
    "    for idx, example in enumerate(split_data):\n",
    "        text = example[\"text\"]\n",
    "        with open(f\"{dataset_path}/{split}.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5d0d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files and initialize trainer\n",
    "files = [fr\"./{dataset_path}/{split}.txt\".replace(\"/\", \"\\\\\").capitalize() for split in [\"test\", \"train\", \"validation\"]]\n",
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0150c934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([42, 1247, 8310, 8035, 1250, 16599, 1176, 3198, 14382, 6114, 0],\n",
       " ['F',\n",
       "  'ist',\n",
       "  'custom',\n",
       "  'trained',\n",
       "  'from',\n",
       "  'scratch',\n",
       "  'to',\n",
       "  'ken',\n",
       "  'izer',\n",
       "  'model',\n",
       "  '[UNK]'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving the trained tokenizer in a file\n",
    "tokenizer_file_path = \"./trained_data/tokenizer.json\"\n",
    "tokenizer.save(tokenizer_file_path)\n",
    "\n",
    "# Now to load our trained tokenizer\n",
    "tokenizer = Tokenizer.from_file(tokenizer_file_path)\n",
    "\n",
    "outputs = tokenizer.encode(\"Fist custom trained from scratch tokenizer model 😁\")\n",
    "\n",
    "outputs.ids, outputs.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d51c370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'😁'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the last ofset in encoded sentence\n",
    "\"Fist custom trained from scratch tokenizer model 😁\"[int(outputs.offsets[-1][0]): int(outputs.offsets[-1][1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28e6b082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'F',\n",
       " 'ist',\n",
       " 'custom',\n",
       " 'trained',\n",
       " 'from',\n",
       " 'scratch',\n",
       " 'to',\n",
       " 'ken',\n",
       " 'izer',\n",
       " 'model',\n",
       " '[UNK]',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To add special token automaticly\n",
    "tokenizer.token_to_id(\"[SEP]\")\n",
    "\n",
    "# to set the post-processing to give us the traditional BERT inputs\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "output = tokenizer.encode(\"Fist custom trained from scratch tokenizer model 😁\")\n",
    "output.tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
