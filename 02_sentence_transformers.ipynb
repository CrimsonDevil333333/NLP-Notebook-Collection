{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentenceTransformer\n",
    "\n",
    "We will use sentence tranformers to genrate our embeddings which can be used in out models latter on....\n",
    "\n",
    "SentenceTransformer is a Python library designed for generating high-quality sentence embeddings using pre-trained transformer-based models. These embeddings capture semantic information from input text, enabling various downstream natural language processing (NLP) tasks such as text classification, semantic search, clustering, and more.\n",
    "\n",
    "#### Pros of SentenceTransformer:\n",
    "Ease of Use: SentenceTransformer provides a simple and user-friendly interface for generating sentence embeddings. With just a few lines of code, users can obtain embeddings for their text data without needing to implement complex models from scratch.\n",
    "\n",
    "Pre-trained Models: The library offers a wide range of pre-trained transformer-based models, including variants of BERT, RoBERTa, DistilBERT, and more. These pre-trained models are trained on large text corpora and have been fine-tuned for various NLP tasks, ensuring high-quality embeddings.\n",
    "\n",
    "Customizable: Users can fine-tune pre-trained models on domain-specific datasets to further improve performance or adapt them to specific tasks. SentenceTransformer provides utilities for fine-tuning models on custom datasets, allowing users to leverage their own data for better embeddings.\n",
    "\n",
    "Support for Multiple Languages: SentenceTransformer supports multiple languages, allowing users to generate embeddings for text in different languages. This makes it suitable for multilingual applications and tasks where text data may be in diverse languages.\n",
    "\n",
    "Compatibility with Deep Learning Frameworks: The library is compatible with popular deep learning frameworks such as PyTorch and TensorFlow, making it easy to integrate sentence embeddings into existing deep learning pipelines or frameworks.\n",
    "\n",
    "Scalability: SentenceTransformer is designed to handle large volumes of text data efficiently. It supports batch processing, allowing users to generate embeddings for multiple sentences simultaneously, which can improve efficiency, especially when dealing with large datasets.\n",
    "\n",
    "State-of-the-Art Performance: The pre-trained transformer models used in SentenceTransformer have achieved state-of-the-art performance on various NLP benchmarks and tasks. By leveraging these models, users can benefit from the latest advancements in NLP research without needing extensive computational resources or expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary imports\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import model\n",
    "# we are using GIST-small-Embedding-v0 as it's accuracy is decent for its traning size\n",
    "# for other model refrences visit https://huggingface.co/spaces/mteb/leaderboard\n",
    "model = SentenceTransformer(\"avsolatorio/GIST-small-Embedding-v0\", device=\"cuda\")\n",
    "\n",
    "# The max_seq_length and word_embedding_dimension are necessary as our vactor databases requre this kind of info!\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,\n",
       " tensor([[-0.0623,  0.0067,  0.0374,  ...,  0.0033,  0.0486,  0.0302],\n",
       "         [-0.0615, -0.0215,  0.0497,  ...,  0.0343,  0.0309, -0.0016],\n",
       "         [-0.0068, -0.0260,  0.0297,  ..., -0.0284, -0.0061, -0.0443],\n",
       "         [-0.0341, -0.0253,  0.0272,  ...,  0.0293,  0.0277,  0.0177]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample embedding genration\n",
    "texts = [\n",
    "    \"who is hello\",\n",
    "    \"my name is hello\",\n",
    "    \"my name is world\",\n",
    "    \"my name is hello world\",\n",
    "    \"my name is hello hello, world\",\n",
    "    \"my name is hello hello, world!\",\n",
    "    \"hello likes to play football and love to play it everyday\",\n",
    "    \"hello last name is world\",\n",
    "    \"hello don't like money\",\n",
    "    \"this will have least similarity in vector cosine similarity\",\n",
    "    \"at least i hope so :(\"\n",
    "]\n",
    "\n",
    "# Compute embeddings\n",
    "embeddings = model.encode(texts, convert_to_tensor=True)\n",
    "len(embeddings[0]), embeddings[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.9138, 0.7433, 0.8490, 0.8624, 0.8552, 0.6799, 0.8527, 0.6773,\n",
       "         0.6498, 0.6154],\n",
       "        [0.9138, 1.0000, 0.8194, 0.9332, 0.9509, 0.9331, 0.7226, 0.8783, 0.6935,\n",
       "         0.6440, 0.6360],\n",
       "        [0.7433, 0.8194, 1.0000, 0.8636, 0.8745, 0.8539, 0.6481, 0.9054, 0.6711,\n",
       "         0.6253, 0.6266],\n",
       "        [0.8490, 0.9332, 0.8636, 1.0000, 0.9775, 0.9640, 0.7184, 0.9076, 0.6884,\n",
       "         0.6298, 0.6285],\n",
       "        [0.8624, 0.9509, 0.8745, 0.9775, 1.0000, 0.9793, 0.7008, 0.9201, 0.6769,\n",
       "         0.6226, 0.6240],\n",
       "        [0.8552, 0.9331, 0.8539, 0.9640, 0.9793, 1.0000, 0.6967, 0.8995, 0.6649,\n",
       "         0.6113, 0.6173],\n",
       "        [0.6799, 0.7226, 0.6481, 0.7184, 0.7008, 0.6967, 1.0000, 0.6781, 0.6814,\n",
       "         0.5969, 0.6297],\n",
       "        [0.8527, 0.8783, 0.9054, 0.9076, 0.9201, 0.8995, 0.6781, 1.0000, 0.6901,\n",
       "         0.6654, 0.6390],\n",
       "        [0.6773, 0.6935, 0.6711, 0.6884, 0.6769, 0.6649, 0.6814, 0.6901, 1.0000,\n",
       "         0.6174, 0.6566],\n",
       "        [0.6498, 0.6440, 0.6253, 0.6298, 0.6226, 0.6113, 0.5969, 0.6654, 0.6174,\n",
       "         1.0000, 0.7023],\n",
       "        [0.6154, 0.6360, 0.6266, 0.6285, 0.6240, 0.6173, 0.6297, 0.6390, 0.6566,\n",
       "         0.7023, 1.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Compute cosine-similarity for each pair of sentences\n",
    "scores = F.cosine_similarity(embeddings.unsqueeze(1), embeddings.unsqueeze(0), dim=-1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
