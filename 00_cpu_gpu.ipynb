{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "190ccc10-86db-48f7-b2e0-f64d3d954319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory :  2047.875 MB\n"
     ]
    }
   ],
   "source": [
    "# Checking if cuda is installed is necessary \n",
    "# Determine whether to use batching based on the availability of CUDA\n",
    "# Batching is advisable on GPU for better performance, as CPU processing\n",
    "# without batching can be inefficient and potentially slower\n",
    "\n",
    "# It is necessary to know as it can impact while actually using/modifing/building new models\n",
    "# to check if cuda drivers are installed  use \"nvcc --version\" in terminal\n",
    "# if not install from official site :)\n",
    "# install pytorch with specific cuda drivers from this website https://pytorch.org/get-started/locally/\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "batch_size = 1\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Available Memory : \",torch.cuda.get_device_properties(0).total_memory/1024/1024, \"MB\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    batch_size = 8\n",
    "else:\n",
    "    print(\"CUDA is not available !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a68efb4-4ed1-4b53-a533-6843666fb35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\satya\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'text': ['When I say this is my favourite film of all time, that comment is not to be taken lightly. I probably watch far too many films than is healthy for me, and have loved quite a few of them. I first saw \"La Femme Nikita\" nearly ten years ago, and it still manages to be my absolute favourite. Why?<br /><br />This is more than an incredibly stylish and sexy thriller. Luc Besson\\'s great flair for impeccable direction, fashion, and appropriate usage of music makes this a very watchable film. But it is Anne Parillaud\\'s perfect rendering of a complex character who transforms from a heartless killer into a compassionate, vibrant young woman that makes this film beautiful. I can\\'t keep my eyes off of her when she is on screen.<br /><br />I have seen several of Luc Besson\\'s films including \"Subway\", \"The Professional\", and the irritating \"Fifth Element\", and \"Nikita\" is without a doubt, far superior to any of these. Although this film has tragic elements, it is ultimately extremely hopeful. It is the story of a person who is cruel and merciless, who ultimately comes to realize her own humanity and her own personal power. That, to me is extremely inspiring. If there is hope for Nikita, there is hope for all of us.',\n",
       "   'I saw this movie because I am a huge fan of the TV series of the same name starring Roy Dupuis and Pet Wilson. The movie was really good and I saw how the TV show is based on the movie. A few episodes of the TV series came directly from the movie and their similarity was amazing. To keep things short, any fan of the movie has to watch the series and any fan of the series must see the original Nikita.',\n",
       "   \"Being that the only foreign films I usually like star a Japanese person in a rubber suit who crushes little tiny buildings and tanks, I had high hopes for this movie. I thought that this was a movie that wouldn't put me to sleep. WRONG! Starts off with a bang, okay, now she's in training, alright, she's an assassin, I'm still with you, oh, now she's having this moral dilemma and she can't decide if she loves her boyfriend or her controller, zzzzz.... Oh well, back to Gamera!\"],\n",
       "  'label': [-1, -1, -1]},\n",
       " <transformers.pipelines.text_classification.TextClassificationPipeline at 0x16e8b7311d0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "import datasets\n",
    "dataset = datasets.load_dataset(\"imdb\", name=\"plain_text\", split=\"unsupervised\")\n",
    "\n",
    "pipe = pipeline(\"text-classification\", \n",
    "                device_map=device, \n",
    "                model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", \n",
    "                revision=\"af0f99b\"\n",
    "               )\n",
    "\n",
    "dataset[1:4], pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "829f06aa-654c-4fcf-9422-9b12259c239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'POSITIVE', 'score': 0.573521077632904}\n",
      "{'label': 'POSITIVE', 'score': 0.9997995495796204}\n",
      "{'label': 'POSITIVE', 'score': 0.9996894598007202}\n",
      "{'label': 'NEGATIVE', 'score': 0.9103317856788635}\n",
      "{'label': 'NEGATIVE', 'score': 0.9979425072669983}\n",
      "{'label': 'NEGATIVE', 'score': 0.9977090358734131}\n",
      "{'label': 'POSITIVE', 'score': 0.9988546371459961}\n",
      "{'label': 'POSITIVE', 'score': 0.9998350143432617}\n",
      "{'label': 'POSITIVE', 'score': 0.9863672852516174}\n",
      "{'label': 'POSITIVE', 'score': 0.9990979433059692}\n",
      "{'label': 'NEGATIVE', 'score': 0.9988611936569214}\n",
      "{'label': 'NEGATIVE', 'score': 0.7546835541725159}\n",
      "{'label': 'POSITIVE', 'score': 0.9954826831817627}\n",
      "{'label': 'POSITIVE', 'score': 0.9992594122886658}\n",
      "{'label': 'POSITIVE', 'score': 0.9948545694351196}\n",
      "{'label': 'NEGATIVE', 'score': 0.9996827840805054}\n",
      "{'label': 'POSITIVE', 'score': 0.999333918094635}\n",
      "{'label': 'NEGATIVE', 'score': 0.7249533534049988}\n",
      "{'label': 'POSITIVE', 'score': 0.8955302238464355}\n",
      "{'label': 'POSITIVE', 'score': 0.9941614270210266}\n",
      "{'label': 'NEGATIVE', 'score': 0.9993002414703369}\n",
      "{'label': 'POSITIVE', 'score': 0.9932733774185181}\n",
      "{'label': 'POSITIVE', 'score': 0.9977446794509888}\n",
      "{'label': 'NEGATIVE', 'score': 0.9934248924255371}\n",
      "{'label': 'POSITIVE', 'score': 0.7540936470031738}\n",
      "{'label': 'POSITIVE', 'score': 0.9969323873519897}\n",
      "{'label': 'NEGATIVE', 'score': 0.9975661039352417}\n",
      "{'label': 'NEGATIVE', 'score': 0.9986681938171387}\n",
      "{'label': 'POSITIVE', 'score': 0.9996500015258789}\n",
      "{'label': 'NEGATIVE', 'score': 0.9997075200080872}\n",
      "{'label': 'NEGATIVE', 'score': 0.9869157075881958}\n",
      "{'label': 'NEGATIVE', 'score': 0.9846365451812744}\n",
      "{'label': 'NEGATIVE', 'score': 0.9993927478790283}\n",
      "{'label': 'POSITIVE', 'score': 0.9993690848350525}\n",
      "{'label': 'POSITIVE', 'score': 0.9938533306121826}\n",
      "{'label': 'POSITIVE', 'score': 0.9985712766647339}\n",
      "{'label': 'NEGATIVE', 'score': 0.9964244961738586}\n",
      "{'label': 'NEGATIVE', 'score': 0.9978469610214233}\n",
      "{'label': 'POSITIVE', 'score': 0.614988386631012}\n",
      "{'label': 'POSITIVE', 'score': 0.9340111017227173}\n",
      "{'label': 'POSITIVE', 'score': 0.9993835687637329}\n",
      "{'label': 'POSITIVE', 'score': 0.9978803396224976}\n",
      "{'label': 'NEGATIVE', 'score': 0.9979458451271057}\n",
      "{'label': 'POSITIVE', 'score': 0.8828821182250977}\n",
      "{'label': 'NEGATIVE', 'score': 0.998021125793457}\n",
      "{'label': 'NEGATIVE', 'score': 0.9963136315345764}\n",
      "{'label': 'NEGATIVE', 'score': 0.9680740833282471}\n",
      "{'label': 'NEGATIVE', 'score': 0.9982329607009888}\n",
      "{'label': 'POSITIVE', 'score': 0.9903361797332764}\n",
      "{'label': 'POSITIVE', 'score': 0.9917961359024048}\n",
      "{'label': 'NEGATIVE', 'score': 0.9997889399528503}\n",
      "{'label': 'NEGATIVE', 'score': 0.9992750287055969}\n",
      "{'label': 'NEGATIVE', 'score': 0.9976535439491272}\n",
      "{'label': 'POSITIVE', 'score': 0.9996069073677063}\n",
      "{'label': 'POSITIVE', 'score': 0.9987795948982239}\n",
      "{'label': 'POSITIVE', 'score': 0.9994959831237793}\n",
      "{'label': 'NEGATIVE', 'score': 0.9263112545013428}\n",
      "{'label': 'NEGATIVE', 'score': 0.99679034948349}\n",
      "{'label': 'NEGATIVE', 'score': 0.9263112545013428}\n",
      "{'label': 'NEGATIVE', 'score': 0.9966811537742615}\n",
      "{'label': 'NEGATIVE', 'score': 0.9995488524436951}\n",
      "{'label': 'POSITIVE', 'score': 0.9990739822387695}\n",
      "{'label': 'POSITIVE', 'score': 0.9995627999305725}\n",
      "{'label': 'POSITIVE', 'score': 0.9996898174285889}\n"
     ]
    }
   ],
   "source": [
    "trimmed_dataset = dataset[:64]\n",
    "for out in pipe(trimmed_dataset[\"text\"], batch_size=batch_size, truncation=\"only_first\"):\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d1ad1d-32fd-4608-a31f-bb479d2a7a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5866fb48-dcf2-42b2-b3e0-408e74cc5e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Streaming batch_size=1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55452adc7d41489abe53e9cc303cb725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Streaming batch_size=8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b84ab681d89e47409db50ba8ba02a8da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Streaming batch_size=64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab02a82d179544308f70810eeae40d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Streaming batch_size=256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc61c4a5f5741e8a53f08d030dc8d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# * Run only if GPU is enabled *\n",
    "# Dataset with no compute\n",
    "class MyDataset(Dataset):\n",
    "    def __len__(self):\n",
    "        return 1000\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return \"This is a test\"\n",
    "\n",
    "\n",
    "dataset = MyDataset()\n",
    "for batch_size in [1, 8, 64, 256]:\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Streaming batch_size={batch_size}\")\n",
    "    for out in tqdm(pipe(dataset, batch_size=batch_size), total=len(dataset)):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4864a971-1a01-4dde-ac9a-c5a47f8efbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Streaming batch_size=1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4e62f4dd6b40c5bc6fdba582715c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Streaming batch_size=8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0befb010df0e4ccc9737815ba6ffaf42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Streaming batch_size=64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5888d94baf74a2aa54c3d26b0a3f508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreaming batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\base.py:1103\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   1102\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m-> 1103\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_tensor_on_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFramework \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\base.py:1001\u001b[0m, in \u001b[0;36mPipeline._ensure_tensor_on_device\u001b[1;34m(self, inputs, device)\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ensure_tensor_on_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, device):\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, ModelOutput):\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ModelOutput(\n\u001b[1;32m-> 1001\u001b[0m             \u001b[43m{\u001b[49m\u001b[43mname\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_tensor_on_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m   1002\u001b[0m         )\n\u001b[0;32m   1003\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m   1004\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(tensor, device) \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\base.py:1001\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ensure_tensor_on_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, device):\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, ModelOutput):\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ModelOutput(\n\u001b[1;32m-> 1001\u001b[0m             {name: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_tensor_on_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m   1002\u001b[0m         )\n\u001b[0;32m   1003\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m   1004\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(tensor, device) \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\base.py:1014\u001b[0m, in \u001b[0;36mPipeline._ensure_tensor_on_device\u001b[1;34m(self, inputs, device)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m {torch\u001b[38;5;241m.\u001b[39mfloat16, torch\u001b[38;5;241m.\u001b[39mbfloat16}:\n\u001b[0;32m   1013\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m-> 1014\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # uncomment to try \n",
    "# # another dataset but with some computation\n",
    "\n",
    "# class MyDataset(Dataset):\n",
    "#     def __len__(self):\n",
    "#         return 1000\n",
    "\n",
    "#     def __getitem__(self, i):\n",
    "#         if i % 64 == 0:\n",
    "#             n = 100\n",
    "#         else:\n",
    "#             n = 1\n",
    "#         return \"This is a test\" * n\n",
    "\n",
    "# dataset = MyDataset()\n",
    "\n",
    "# for batch_size in [1, 8, 64, 256]:\n",
    "#     print(\"-\" * 30)\n",
    "#     print(f\"Streaming batch_size={batch_size}\")\n",
    "#     for out in tqdm(pipe(dataset, batch_size=batch_size), total=len(dataset)):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc564af-fa65-4e04-992a-c76aeb42bfb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
